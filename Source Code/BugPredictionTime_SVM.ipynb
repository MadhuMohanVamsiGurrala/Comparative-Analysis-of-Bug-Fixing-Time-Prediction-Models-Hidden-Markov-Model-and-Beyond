{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ea14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b26001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug ID</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Product</th>\n",
       "      <th>Component</th>\n",
       "      <th>Assignee</th>\n",
       "      <th>Status</th>\n",
       "      <th>Notification</th>\n",
       "      <th>Updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1116989</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>Guest users should not see introduction info</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Toolbars and Customization</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1/2/2015 12:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1047673</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>URL checksums (for \"securely\" linking third-pa...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>General</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1/2/2015 14:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108341</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Context menu entries use the original URI inst...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Menus</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1/3/2015 8:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1111278</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>A strip with all(?) possible icons is showing ...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Toolbars and Customization</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>High</td>\n",
       "      <td>1/5/2015 22:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>992209</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>invoking firefox with command line argument(s)...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>General</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>High</td>\n",
       "      <td>1/6/2015 5:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bug ID     Reporting                                            Summary  \\\n",
       "0  1116989      Beginner       Guest users should not see introduction info   \n",
       "1  1047673      Beginner  URL checksums (for \"securely\" linking third-pa...   \n",
       "2  1108341  Intermediate  Context menu entries use the original URI inst...   \n",
       "3  1111278  Intermediate  A strip with all(?) possible icons is showing ...   \n",
       "4   992209      Beginner  invoking firefox with command line argument(s)...   \n",
       "\n",
       "   Product                   Component Assignee       Status Notification  \\\n",
       "0  Firefox  Toolbars and Customization   Random  Unconfirmed       Normal   \n",
       "1  Firefox                     General   Random  Unconfirmed       Normal   \n",
       "2  Firefox                       Menus   Random  Unconfirmed       Normal   \n",
       "3  Firefox  Toolbars and Customization   Random  Unconfirmed         High   \n",
       "4  Firefox                     General   Random  Unconfirmed         High   \n",
       "\n",
       "          Updated  \n",
       "0  1/2/2015 12:31  \n",
       "1  1/2/2015 14:16  \n",
       "2   1/3/2015 8:26  \n",
       "3  1/5/2015 22:18  \n",
       "4   1/6/2015 5:17  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing dataset\n",
    "bugs=pd.read_csv(r\"E:\\Project\\Firefox_Bugs_data.csv\")\n",
    "bugs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7e55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning integer values to values in Reporting column\n",
    "def trans_reporting(x):\n",
    "    if x == 'Beginner':\n",
    "        return 0\n",
    "    if x == 'Intermediate':\n",
    "        return 1\n",
    "    if x == 'Advanced':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ffef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning integer values to values in Assignee column\n",
    "def trans_assignee(x):\n",
    "    if x == 'Particular':\n",
    "        return 0\n",
    "    if x == 'Random':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning integer values to values in Status column\n",
    "def trans_status(x):\n",
    "    if x == 'Unconfirmed':\n",
    "        return 0\n",
    "    if x == 'New':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc59921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning integer values to values in Notification column\n",
    "def trans_notification(x):\n",
    "    if x == 'Normal':\n",
    "        return 0\n",
    "    if x == 'High':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9648fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deec3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying these values to Firefox_bugs dataset\n",
    "bugs['Trans_Reporting']=bugs['Reporting'].apply(trans_reporting)\n",
    "bugs['Trans_Assignee']=bugs['Assignee'].apply(trans_assignee)\n",
    "bugs['Trans_Status']=bugs['Status'].apply(trans_status)\n",
    "bugs['Trans_Notification']=bugs['Notification'].apply(trans_notification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0131ba2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug ID</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Product</th>\n",
       "      <th>Component</th>\n",
       "      <th>Assignee</th>\n",
       "      <th>Status</th>\n",
       "      <th>Notification</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Trans_Reporting</th>\n",
       "      <th>Trans_Assignee</th>\n",
       "      <th>Trans_Status</th>\n",
       "      <th>Trans_Notification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1116989</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>Guest users should not see introduction info</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Toolbars and Customization</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2015-01-02 12:31:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1047673</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>URL checksums (for \"securely\" linking third-pa...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>General</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2015-01-02 14:16:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108341</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Context menu entries use the original URI inst...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Menus</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2015-01-03 08:26:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1111278</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>A strip with all(?) possible icons is showing ...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Toolbars and Customization</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>High</td>\n",
       "      <td>2015-01-05 22:18:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>992209</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>invoking firefox with command line argument(s)...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>General</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>High</td>\n",
       "      <td>2015-01-06 05:17:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bug ID     Reporting                                            Summary  \\\n",
       "0  1116989      Beginner       Guest users should not see introduction info   \n",
       "1  1047673      Beginner  URL checksums (for \"securely\" linking third-pa...   \n",
       "2  1108341  Intermediate  Context menu entries use the original URI inst...   \n",
       "3  1111278  Intermediate  A strip with all(?) possible icons is showing ...   \n",
       "4   992209      Beginner  invoking firefox with command line argument(s)...   \n",
       "\n",
       "   Product                   Component Assignee       Status Notification  \\\n",
       "0  Firefox  Toolbars and Customization   Random  Unconfirmed       Normal   \n",
       "1  Firefox                     General   Random  Unconfirmed       Normal   \n",
       "2  Firefox                       Menus   Random  Unconfirmed       Normal   \n",
       "3  Firefox  Toolbars and Customization   Random  Unconfirmed         High   \n",
       "4  Firefox                     General   Random  Unconfirmed         High   \n",
       "\n",
       "              Updated  Trans_Reporting  Trans_Assignee  Trans_Status  \\\n",
       "0 2015-01-02 12:31:00                0               1             0   \n",
       "1 2015-01-02 14:16:00                0               1             0   \n",
       "2 2015-01-03 08:26:00                1               1             0   \n",
       "3 2015-01-05 22:18:00                1               1             0   \n",
       "4 2015-01-06 05:17:00                0               1             0   \n",
       "\n",
       "   Trans_Notification  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing date-time\n",
    "bugs[\"Updated\"] = pd.to_datetime(bugs[\"Updated\"])\n",
    "bugs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8be133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug ID</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Product</th>\n",
       "      <th>Component</th>\n",
       "      <th>Assignee</th>\n",
       "      <th>Status</th>\n",
       "      <th>Notification</th>\n",
       "      <th>Trans_Reporting</th>\n",
       "      <th>Trans_Assignee</th>\n",
       "      <th>Trans_Status</th>\n",
       "      <th>Trans_Notification</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Updated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02 12:31:00</th>\n",
       "      <td>1116989</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>Guest users should not see introduction info</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Toolbars and Customization</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 14:16:00</th>\n",
       "      <td>1047673</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>URL checksums (for \"securely\" linking third-pa...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>General</td>\n",
       "      <td>Random</td>\n",
       "      <td>Unconfirmed</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Bug ID Reporting  \\\n",
       "Updated                                  \n",
       "2015-01-02 12:31:00  1116989  Beginner   \n",
       "2015-01-02 14:16:00  1047673  Beginner   \n",
       "\n",
       "                                                               Summary  \\\n",
       "Updated                                                                  \n",
       "2015-01-02 12:31:00       Guest users should not see introduction info   \n",
       "2015-01-02 14:16:00  URL checksums (for \"securely\" linking third-pa...   \n",
       "\n",
       "                     Product                   Component Assignee  \\\n",
       "Updated                                                             \n",
       "2015-01-02 12:31:00  Firefox  Toolbars and Customization   Random   \n",
       "2015-01-02 14:16:00  Firefox                     General   Random   \n",
       "\n",
       "                          Status Notification  Trans_Reporting  \\\n",
       "Updated                                                          \n",
       "2015-01-02 12:31:00  Unconfirmed       Normal                0   \n",
       "2015-01-02 14:16:00  Unconfirmed       Normal                0   \n",
       "\n",
       "                     Trans_Assignee  Trans_Status  Trans_Notification  \n",
       "Updated                                                                \n",
       "2015-01-02 12:31:00               1             0                   0  \n",
       "2015-01-02 14:16:00               1             0                   0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assigning index for Updated column\n",
    "bugs.set_index(\"Updated\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3564e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping bugs in 2015 into one dataset\n",
    "Firefox_bugs_2015 = bugs[bugs[\"Updated\"].between('2015-01-02','2016-01-01')]\n",
    "#grouping bugs in 2016 into one dataset\n",
    "Firefox_bugs_2016 = bugs[bugs[\"Updated\"].between('2016-01-01','2017-01-01')]\n",
    "#grouping bugs in 2017 into one dataset\n",
    "Firefox_bugs_2017 = bugs[bugs[\"Updated\"].between('2017-01-01','2018-01-03')]\n",
    "#grouping bugs in 2018 into one dataset\n",
    "Firefox_bugs_2018 = bugs[bugs[\"Updated\"].between('2018-01-03','2019-01-02')]\n",
    "#grouping bugs in 2019 into one dataset\n",
    "Firefox_bugs_2019 = bugs[bugs[\"Updated\"].between('2019-01-02','2020-01-02')]\n",
    "#grouping bugs in 2020 into one dataset\n",
    "Firefox_bugs_2020 = bugs[bugs[\"Updated\"].between('2020-01-02','2021-04-01')]\n",
    "#grouping bugs in 2021 into one dataset\n",
    "Firefox_bugs_2021 = bugs[bugs[\"Updated\"].between('2021-04-01','2022-01-01')]\n",
    "#grouping bugs in 2022 into one dataset\n",
    "Firefox_bugs_2022 = bugs[bugs[\"Updated\"].between('2022-01-01','2022-11-09')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e03f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying these values to Firefox_bugs_2015 dataset\n",
    "Firefox_bugs_2015['Trans_Reporting']=Firefox_bugs_2015['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2015['Trans_Assignee']=Firefox_bugs_2015['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2015['Trans_Status']=Firefox_bugs_2015['Status'].apply(trans_status)\n",
    "Firefox_bugs_2015['Trans_Notification']=Firefox_bugs_2015['Notification'].apply(trans_notification)\n",
    "#applying these values to Firefox_bugs_2016 dataset\n",
    "Firefox_bugs_2016['Trans_Reporting']=Firefox_bugs_2016['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2016['Trans_Assignee']=Firefox_bugs_2016['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2016['Trans_Status']=Firefox_bugs_2016['Status'].apply(trans_status)\n",
    "Firefox_bugs_2016['Trans_Notification']=Firefox_bugs_2016['Notification'].apply(trans_notification)\n",
    "#applying these values to Firefox_bugs_2017 dataset\n",
    "Firefox_bugs_2017['Trans_Reporting']=Firefox_bugs_2017['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2017['Trans_Assignee']=Firefox_bugs_2017['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2017['Trans_Status']=Firefox_bugs_2017['Status'].apply(trans_status)\n",
    "Firefox_bugs_2017['Trans_Notification']=Firefox_bugs_2017['Notification'].apply(trans_notification)\n",
    "#applying these values to Firefox_bugs_2018 dataset\n",
    "Firefox_bugs_2018['Trans_Reporting']=Firefox_bugs_2018['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2018['Trans_Assignee']=Firefox_bugs_2018['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2018['Trans_Status']=Firefox_bugs_2018['Status'].apply(trans_status)\n",
    "Firefox_bugs_2018['Trans_Notification']=Firefox_bugs_2018['Notification'].apply(trans_notification)\n",
    "Firefox_bugs_2018.head(5)\n",
    "#applying these values to Firefox_bugs_2019 dataset\n",
    "Firefox_bugs_2019['Trans_Reporting']=Firefox_bugs_2019['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2019['Trans_Assignee']=Firefox_bugs_2019['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2019['Trans_Status']=Firefox_bugs_2019['Status'].apply(trans_status)\n",
    "Firefox_bugs_2019['Trans_Notification']=Firefox_bugs_2019['Notification'].apply(trans_notification)\n",
    "#applying these values to Firefox_bugs_2020 dataset\n",
    "Firefox_bugs_2020['Trans_Reporting']=Firefox_bugs_2020['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2020['Trans_Assignee']=Firefox_bugs_2020['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2020['Trans_Status']=Firefox_bugs_2020['Status'].apply(trans_status)\n",
    "Firefox_bugs_2020['Trans_Notification']=Firefox_bugs_2020['Notification'].apply(trans_notification)\n",
    "#applying these values to Firefox_bugs_2021 dataset\n",
    "Firefox_bugs_2021['Trans_Reporting']=Firefox_bugs_2021['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2021['Trans_Assignee']=Firefox_bugs_2021['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2021['Trans_Status']=Firefox_bugs_2021['Status'].apply(trans_status)\n",
    "Firefox_bugs_2021['Trans_Notification']=Firefox_bugs_2021['Notification'].apply(trans_notification)\n",
    "#applying these values to Firefox_bugs_2022 dataset\n",
    "Firefox_bugs_2022['Trans_Reporting']=Firefox_bugs_2022['Reporting'].apply(trans_reporting)\n",
    "Firefox_bugs_2022['Trans_Assignee']=Firefox_bugs_2022['Assignee'].apply(trans_assignee)\n",
    "Firefox_bugs_2022['Trans_Status']=Firefox_bugs_2022['Status'].apply(trans_status)\n",
    "Firefox_bugs_2022['Trans_Notification']=Firefox_bugs_2022['Notification'].apply(trans_notification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3477955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assingning values to x1,x2,x3,x4,x5,x6,x7,x8 and y1,y2,y3,y4,y5,y6,y7,y8\n",
    "X1=Firefox_bugs_2015.iloc[:,9:12].values\n",
    "y1=Firefox_bugs_2015.iloc[:,12:13].values\n",
    "X2=Firefox_bugs_2016.iloc[:,9:12].values\n",
    "y2=Firefox_bugs_2016.iloc[:,12:13].values\n",
    "X3=Firefox_bugs_2017.iloc[:,9:12].values\n",
    "y3=Firefox_bugs_2017.iloc[:,12:13].values\n",
    "X4=Firefox_bugs_2018.iloc[:,9:12].values\n",
    "y4=Firefox_bugs_2018.iloc[:,12:13].values\n",
    "X5=Firefox_bugs_2019.iloc[:,9:12].values\n",
    "y5=Firefox_bugs_2019.iloc[:,12:13].values\n",
    "X6=Firefox_bugs_2020.iloc[:,9:12].values\n",
    "y6=Firefox_bugs_2020.iloc[:,12:13].values\n",
    "X7=Firefox_bugs_2021.iloc[:,9:12].values\n",
    "y7=Firefox_bugs_2021.iloc[:,12:13].values\n",
    "X8=Firefox_bugs_2022.iloc[:,9:12].values\n",
    "y8=Firefox_bugs_2022.iloc[:,12:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f35e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diving the dataset into training and testing data\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1,test_size=0.2)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2,test_size=0.2)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3,y3,test_size=0.2)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4,y4,test_size=0.2)\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X5,y5,test_size=0.2)\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X6,y6,test_size=0.2)\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X7,y7,test_size=0.2)\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X8,y8,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8ce8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assingning values to x and y\n",
    "X=bugs.iloc[:,9:12].values\n",
    "y=bugs.iloc[:,12:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b3c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diving the dataset into 80% training and 20% testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "798b264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7246\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6787\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6704\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7320\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.6799\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6741\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6762\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7481\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6741\n",
      "Fold 2: 0.7261\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.6762\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6650\n",
      "Fold 8: 0.7457\n",
      "Fold 9: 0.7196\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.7258\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.6787\n",
      "Fold 5: 0.7370\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.7270\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.7348\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7258\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Mean Scores for SVM: [0.70210149 0.70209565 0.70209565 0.70210257 0.70209704 0.70210042\n",
      " 0.7020975  0.70209058 0.70209596 0.70209688]\n",
      "Standard Deviations for SVM: [0.01315968 0.01144258 0.02034744 0.02076061 0.0242528  0.012973\n",
      " 0.02069351 0.01391791 0.01007008 0.00990941]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abe8a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020973737858259\n",
      "Standard Deviations: 0.016514117685305956\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_1 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_1 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_1)\n",
    "print(\"Standard Deviations:\", svm_std_scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "873c698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6625\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6778\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7320\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7249\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6774\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6704\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7320\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6762\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.7134\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6712\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.7221\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.6725\n",
      "Fold 5: 0.7345\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Mean Scores for SVM: [0.70209704 0.70209888 0.70209581 0.70209442 0.70209657 0.70209888\n",
      " 0.70209734 0.70196851 0.70209673 0.70209135]\n",
      "Standard Deviations for SVM: [0.01540167 0.01405478 0.00866509 0.01269095 0.0145007  0.01317983\n",
      " 0.01249204 0.01408428 0.01642297 0.01998583]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c162654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020835524151269\n",
      "Standard Deviations: 0.01441683325813612\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_2 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_2 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_2)\n",
    "print(\"Standard Deviations:\", svm_std_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e50ace45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6766\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.6799\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7283\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7212\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6712\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6803\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7283\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7382\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6914\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7249\n",
      "Fold 3: 0.6778\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7308\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6791\n",
      "Fold 4: 0.6737\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7283\n",
      "Fold 7: 0.7208\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7361\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.7286\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.6725\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Mean Scores for SVM: [0.7020995  0.70209427 0.70209919 0.70209704 0.70209873 0.70209642\n",
      " 0.70210026 0.70209258 0.70209565 0.70209119]\n",
      "Standard Deviations for SVM: [0.0157171  0.01350787 0.0130847  0.01615236 0.00882911 0.01720931\n",
      " 0.01720849 0.01523904 0.01112373 0.0178777 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb1a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020964820844902\n",
      "Standard Deviations: 0.014857393211007831\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_3 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_3 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_3)\n",
    "print(\"Standard Deviations:\", svm_std_scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0adc0136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7283\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.6811\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7382\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.7233\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.6753\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.7208\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7274\n",
      "Fold 3: 0.7249\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7308\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.6663\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6712\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7246\n",
      "Fold 5: 0.6600\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.6716\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7196\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7270\n",
      "\n",
      "Mean Scores for SVM: [0.70209258 0.7020998  0.70209611 0.70209642 0.70210011 0.70209288\n",
      " 0.70209288 0.7020935  0.70210211 0.70210042]\n",
      "Standard Deviations for SVM: [0.0148761  0.01595451 0.01194194 0.01320646 0.01525099 0.0213123\n",
      " 0.01437529 0.01653818 0.01466832 0.01637616]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54fc26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020966819485828\n",
      "Standard Deviations: 0.015630643495323483\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_4 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_4 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_4)\n",
    "print(\"Standard Deviations:\", svm_std_scores_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5103467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.6762\n",
      "Fold 5: 0.7233\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.6753\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7299\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7175\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6700\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7283\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7295\n",
      "Fold 6: 0.7457\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.6725\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6753\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.6836\n",
      "Fold 6: 0.7233\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7357\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.6749\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6687\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Mean Scores for SVM: [0.70209673 0.7020995  0.70209581 0.70209903 0.70209396 0.70209827\n",
      " 0.70210103 0.70209304 0.70209396 0.7020975 ]\n",
      "Standard Deviations for SVM: [0.01284625 0.0119435  0.01353631 0.0137713  0.01557916 0.02048485\n",
      " 0.01853771 0.01245546 0.01461963 0.0112949 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2eaabfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702096881812675\n",
      "Standard Deviations: 0.014774934430435633\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_5 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_5 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_5)\n",
    "print(\"Standard Deviations:\", svm_std_scores_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d9ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6815\n",
      "Fold 2: 0.7261\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.6836\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6787\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7270\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.6853\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7407\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6787\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.6762\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.7208\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6815\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6815\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6741\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.6592\n",
      "Fold 3: 0.7336\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7258\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6774\n",
      "\n",
      "Mean Scores for SVM: [0.7020978  0.70209657 0.70209765 0.7020935  0.70209396 0.70209934\n",
      " 0.70209842 0.70209642 0.70209427 0.70209534]\n",
      "Standard Deviations for SVM: [0.01501321 0.01375184 0.00932159 0.01686946 0.01628829 0.0109128\n",
      " 0.01071121 0.01387028 0.01653048 0.02149277]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f7c2187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020963283428806\n",
      "Standard Deviations: 0.01487510855727549\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_6 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_6 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_6)\n",
    "print(\"Standard Deviations:\", svm_std_scores_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bd47657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.6687\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.6791\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6716\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7333\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7283\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.7357\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6774\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7258\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7233\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6803\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.7283\n",
      "Fold 6: 0.7283\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.6700\n",
      "Fold 5: 0.7233\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7308\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Mean Scores for SVM: [0.70209519 0.70209873 0.70210272 0.70209596 0.70209396 0.70209504\n",
      " 0.70209704 0.70209903 0.70209888 0.7020955 ]\n",
      "Standard Deviations for SVM: [0.01509516 0.01094291 0.0183663  0.01031546 0.01674206 0.00904066\n",
      " 0.01459164 0.0136307  0.01569103 0.01642792]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e33d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702097204670055\n",
      "Standard Deviations: 0.014381126483736727\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_7 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_7 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_7)\n",
    "print(\"Standard Deviations:\", svm_std_scores_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66b01786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7246\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6774\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7246\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6778\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7336\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.6778\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7270\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.7208\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7208\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6737\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6811\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.7395\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6753\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.6836\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7345\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6803\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.7270\n",
      "\n",
      "Mean Scores for SVM: [0.70209765 0.70209565 0.7020978  0.70210119 0.7020975  0.70209258\n",
      " 0.7020955  0.70209796 0.70209934 0.7020955 ]\n",
      "Standard Deviations for SVM: [0.01189028 0.01192483 0.01348895 0.01259204 0.01846277 0.01219465\n",
      " 0.01156557 0.01809738 0.01723295 0.01570343]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d9f35d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020970663026066\n",
      "Standard Deviations: 0.014555153071886128\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_8 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_8 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_8)\n",
    "print(\"Standard Deviations:\", svm_std_scores_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15a70dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7320\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7274\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.6811\n",
      "Fold 8: 0.7407\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.6811\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.6853\n",
      "Fold 3: 0.7274\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.7208\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.7295\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.6749\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.7345\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6712\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7261\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7407\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7382\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.6762\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7457\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Mean Scores for SVM: [0.70209842 0.70209611 0.70209473 0.70209519 0.7020955  0.70209611\n",
      " 0.70209365 0.70209704 0.70209642 0.70209719]\n",
      "Standard Deviations for SVM: [0.01184684 0.01973947 0.01559284 0.0153015  0.01598718 0.01660345\n",
      " 0.02087914 0.01798747 0.01848441 0.01296063]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da02a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020960362338226\n",
      "Standard Deviations: 0.016756105107580563\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_9 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_9 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_9)\n",
    "print(\"Standard Deviations:\", svm_std_scores_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9be65519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6815\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7258\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.7261\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7320\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6914\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7221\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.7246\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7295\n",
      "Fold 10: 0.6749\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.6815\n",
      "Fold 4: 0.6588\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7320\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6824\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7233\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Mean Scores for SVM: [0.70209504 0.70209811 0.70209396 0.70209796 0.70209842 0.70209688\n",
      " 0.70209365 0.70209888 0.70209719 0.70209734]\n",
      "Standard Deviations for SVM: [0.01409036 0.00964714 0.01551718 0.01369798 0.01749024 0.01972315\n",
      " 0.01038095 0.01363317 0.0097544  0.010246  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93d91728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020967434452264\n",
      "Standard Deviations: 0.013815538747284516\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_10 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_10 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_10)\n",
    "print(\"Standard Deviations:\", svm_std_scores_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "696fb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7311\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7258\n",
      "Fold 7: 0.6675\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.6762\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7320\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6712\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.6853\n",
      "Fold 3: 0.6716\n",
      "Fold 4: 0.7283\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6638\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.7295\n",
      "Fold 7: 0.7258\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7233\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7221\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7233\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7357\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.6663\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7308\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.6774\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Mean Scores for SVM: [0.70209319 0.70209165 0.70210119 0.7020955  0.70209657 0.7020975\n",
      " 0.70209504 0.70209381 0.70209719 0.70209811]\n",
      "Standard Deviations for SVM: [0.01696061 0.01771836 0.01461329 0.0185647  0.01470231 0.01083664\n",
      " 0.01085151 0.01015243 0.02039609 0.01390938]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aef6495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020959747371787\n",
      "Standard Deviations: 0.01524364906970056\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_11 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_11 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_11)\n",
    "print(\"Standard Deviations:\", svm_std_scores_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee44550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.7299\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.6638\n",
      "Fold 6: 0.6737\n",
      "Fold 7: 0.7171\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.6811\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.7258\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6716\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6787\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.7221\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.6774\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.7233\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7320\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.6663\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7419\n",
      "Fold 10: 0.6725\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7311\n",
      "Fold 2: 0.6791\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.6675\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.7320\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.7333\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6679\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Mean Scores for SVM: [0.7020975  0.70209273 0.70209765 0.70209934 0.70210088 0.70209611\n",
      " 0.70209673 0.70209488 0.70209288 0.70210134]\n",
      "Standard Deviations for SVM: [0.00958922 0.02039416 0.00491343 0.01471071 0.01581456 0.0142469\n",
      " 0.01451078 0.0215025  0.02348378 0.01554835]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2786ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020970048059628\n",
      "Standard Deviations: 0.01633295858813219\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_12 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_12 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_12)\n",
    "print(\"Standard Deviations:\", svm_std_scores_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e364bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.7274\n",
      "Fold 4: 0.7333\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.6501\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6799\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.6638\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6687\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7295\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6629\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7274\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.6737\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.7258\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7258\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Mean Scores for SVM: [0.70208966 0.70209427 0.70209996 0.70209534 0.70209458 0.70209811\n",
      " 0.70209704 0.70209827 0.7020935  0.7020978 ]\n",
      "Standard Deviations for SVM: [0.02357395 0.01504969 0.01086623 0.01515206 0.00943639 0.01623524\n",
      " 0.01076663 0.01181103 0.01565448 0.01404899]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43fd38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702095851743891\n",
      "Standard Deviations: 0.014766078491622746\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_13 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_13 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_13)\n",
    "print(\"Standard Deviations:\", svm_std_scores_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bad8cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7299\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6725\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6762\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.6753\n",
      "Fold 3: 0.6778\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7270\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7295\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.6629\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7283\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.6737\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.7320\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7333\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6700\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Mean Scores for SVM: [0.70209288 0.7020915  0.70210057 0.70209965 0.70209058 0.70209488\n",
      " 0.70210011 0.70209842 0.70209642 0.70209673]\n",
      "Standard Deviations for SVM: [0.0167103  0.01360218 0.01903262 0.01822888 0.01489493 0.01313693\n",
      " 0.01343649 0.01562183 0.01098364 0.01412626]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12d8aeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702096174601271\n",
      "Standard Deviations: 0.015159145006040304\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_14 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_14 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_14)\n",
    "print(\"Standard Deviations:\", svm_std_scores_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39d6439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.7295\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6811\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7345\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6712\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.6774\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6914\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6749\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7261\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7134\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.6725\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6600\n",
      "Fold 10: 0.7295\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7311\n",
      "Fold 2: 0.6654\n",
      "Fold 3: 0.6617\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7308\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7345\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Mean Scores for SVM: [0.70209488 0.70209765 0.70209704 0.70209442 0.70209396 0.70209473\n",
      " 0.70209611 0.70209581 0.70209627 0.70210196]\n",
      "Standard Deviations for SVM: [0.01566908 0.00869225 0.0177109  0.01366002 0.01610823 0.01403392\n",
      " 0.01202587 0.02172792 0.00778715 0.02513488]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1b006ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702096282220398\n",
      "Standard Deviations: 0.01608384222842572\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_15 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_15 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_15)\n",
    "print(\"Standard Deviations:\", svm_std_scores_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "333f7142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7373\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7283\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.6749\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7196\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.6766\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6787\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7295\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.6737\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7382\n",
      "Fold 10: 0.6588\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7270\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7261\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6675\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7506\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.6600\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6642\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.7295\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7233\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.7274\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.7246\n",
      "\n",
      "Mean Scores for SVM: [0.70209273 0.70209688 0.70209827 0.70209719 0.70209611 0.70209488\n",
      " 0.70209504 0.70210042 0.70209903 0.70209212]\n",
      "Standard Deviations for SVM: [0.02055695 0.01223777 0.01250223 0.01570686 0.02386868 0.01189493\n",
      " 0.0255893  0.01913213 0.01174711 0.01642536]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a494a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020962668462368\n",
      "Standard Deviations: 0.017649937647198417\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_16 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_16 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_16)\n",
    "print(\"Standard Deviations:\", svm_std_scores_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b157f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6753\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7274\n",
      "Fold 3: 0.6729\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6691\n",
      "Fold 2: 0.7323\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.6667\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.6774\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7146\n",
      "Fold 10: 0.7308\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6753\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6914\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.7320\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6787\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6799\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6700\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.7283\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Mean Scores for SVM: [0.7020975  0.70209534 0.7020978  0.70210088 0.70210026 0.70209704\n",
      " 0.70209642 0.70209565 0.70209627 0.70209673]\n",
      "Standard Deviations for SVM: [0.01208078 0.01533343 0.01598015 0.01856183 0.01559289 0.01108946\n",
      " 0.00944615 0.00989797 0.01217076 0.01378089]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6598a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020973891599867\n",
      "Standard Deviations: 0.01368327493932222\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_17 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_17 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_17)\n",
    "print(\"Standard Deviations:\", svm_std_scores_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e2d342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.7270\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.6700\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6638\n",
      "Fold 10: 0.7295\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7212\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7221\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.7258\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6687\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.7370\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7146\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.7469\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6762\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6762\n",
      "Fold 9: 0.7320\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7196\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6815\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7320\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.7233\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Mean Scores for SVM: [0.70209596 0.70209611 0.7020998  0.7020975  0.70209857 0.70209627\n",
      " 0.70209565 0.70209704 0.70209657 0.70209934]\n",
      "Standard Deviations for SVM: [0.02142757 0.01626017 0.01778105 0.01217431 0.01636858 0.00991036\n",
      " 0.01146433 0.01811854 0.00999421 0.01430667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9a626c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.70209728154086\n",
      "Standard Deviations: 0.015226085112575035\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_18 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_18 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_18)\n",
    "print(\"Standard Deviations:\", svm_std_scores_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91ce7740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7270\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.6576\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7212\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6811\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6803\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7373\n",
      "Fold 4: 0.7246\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.6613\n",
      "Fold 7: 0.7283\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.6749\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6778\n",
      "Fold 2: 0.7534\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.6749\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.6737\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.6774\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.6600\n",
      "Fold 5: 0.6687\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6650\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Mean Scores for SVM: [0.70209704 0.70209642 0.70209227 0.70209458 0.70209442 0.70209381\n",
      " 0.70209288 0.70209827 0.70209196 0.70209934]\n",
      "Standard Deviations for SVM: [0.01357422 0.02118086 0.01072524 0.01347896 0.02447894 0.02250604\n",
      " 0.01525044 0.01293041 0.02000617 0.0162799 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "155e4d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020950984100045\n",
      "Standard Deviations: 0.017607940399214837\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_19 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_19 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_19)\n",
    "print(\"Standard Deviations:\", svm_std_scores_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b2defbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.6729\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7249\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7295\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7361\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6803\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6811\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.7270\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.7270\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7246\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7246\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6803\n",
      "Fold 2: 0.7286\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.7320\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.7233\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7370\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7311\n",
      "Fold 4: 0.6762\n",
      "Fold 5: 0.7221\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.6762\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7323\n",
      "Fold 3: 0.7311\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.7295\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.6642\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7270\n",
      "Fold 8: 0.7270\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7233\n",
      "\n",
      "Mean Scores for SVM: [0.70209827 0.70209381 0.70209411 0.70210149 0.7020975  0.70209673\n",
      " 0.70209135 0.7020975  0.70208966 0.70210534]\n",
      "Standard Deviations for SVM: [0.01300553 0.01429186 0.01729003 0.01678847 0.01889457 0.01353698\n",
      " 0.01689244 0.01299537 0.0205635  0.02132309]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8934a8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702096574329456\n",
      "Standard Deviations: 0.016811362611735135\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_20 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_20 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_20)\n",
    "print(\"Standard Deviations:\", svm_std_scores_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7c868f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.6729\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7345\n",
      "Fold 7: 0.7320\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6811\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.7357\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6654\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.7308\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7320\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7370\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7348\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6725\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7348\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6675\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7357\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6725\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Mean Scores for SVM: [0.70210119 0.70209534 0.70210395 0.70209319 0.70209596 0.70209581\n",
      " 0.70209104 0.70209919 0.70209319 0.70209811]\n",
      "Standard Deviations for SVM: [0.019449   0.01446681 0.02353115 0.01583217 0.00731089 0.01191302\n",
      " 0.01849765 0.0160242  0.01289245 0.01236095]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ebac9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020966973227438\n",
      "Standard Deviations: 0.01582795170278491\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_21 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_21 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_21)\n",
    "print(\"Standard Deviations:\", svm_std_scores_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68708d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7345\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6712\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7286\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6811\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6787\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.6762\n",
      "Fold 7: 0.7233\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7261\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7221\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7261\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6836\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.6791\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.7208\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7270\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.6762\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7233\n",
      "Fold 5: 0.7233\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.7395\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7581\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.6762\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Mean Scores for SVM: [0.70209335 0.70209212 0.70209227 0.70209319 0.70209504 0.70209873\n",
      " 0.7020975  0.70209965 0.70210057 0.70209273]\n",
      "Standard Deviations for SVM: [0.01968127 0.01522891 0.01480194 0.01530951 0.01274704 0.01365694\n",
      " 0.01460901 0.01473361 0.02503898 0.01263779]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58a2ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020955135123502\n",
      "Standard Deviations: 0.01624474241111219\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_22 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_22 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_22)\n",
    "print(\"Standard Deviations:\", svm_std_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "947be2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6679\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7432\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7283\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.7333\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.6700\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7419\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.7221\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.7134\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7382\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6737\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.7295\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6687\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6836\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7246\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.7212\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7295\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6737\n",
      "Fold 10: 0.6749\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.6803\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Mean Scores for SVM: [0.70210103 0.70209627 0.70210103 0.70210057 0.70209396 0.70209842\n",
      " 0.70209842 0.70209888 0.70209565 0.70209873]\n",
      "Standard Deviations for SVM: [0.01888028 0.01644176 0.01964415 0.01343469 0.01940858 0.01668089\n",
      " 0.01310216 0.01302185 0.02099917 0.01055465]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc263d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702098296235483\n",
      "Standard Deviations: 0.016558159170189757\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_23 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_23 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_23)\n",
    "print(\"Standard Deviations:\", svm_std_scores_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "480e6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7444\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6691\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.7295\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.6799\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.6753\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7283\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7146\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7261\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.6737\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.7270\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6787\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7286\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.7208\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7274\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Mean Scores for SVM: [0.70209719 0.70209534 0.70209596 0.70209796 0.70209719 0.7020915\n",
      " 0.70209012 0.70209534 0.70209411 0.7020915 ]\n",
      "Standard Deviations for SVM: [0.01414706 0.01739099 0.00824059 0.01885394 0.01493537 0.01788397\n",
      " 0.01427984 0.01052569 0.01360306 0.01420917]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7e20414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020946218110146\n",
      "Standard Deviations: 0.014732664365182642\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_24 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_24 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_24)\n",
    "print(\"Standard Deviations:\", svm_std_scores_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52e1b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6617\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7196\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.6762\n",
      "Fold 5: 0.7308\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7308\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6749\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6749\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.7184\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6824\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.7295\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7212\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.6712\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7407\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6799\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Mean Scores for SVM: [0.70209903 0.70209534 0.70209704 0.70209565 0.70209304 0.70209242\n",
      " 0.70209565 0.7020955  0.70209596 0.70209765]\n",
      "Standard Deviations for SVM: [0.01528017 0.01256247 0.0203561  0.01041529 0.01444986 0.01339793\n",
      " 0.01474485 0.01464027 0.01144751 0.015411  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f758cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020957287506034\n",
      "Standard Deviations: 0.014500840058817395\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_25 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_25 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_25)\n",
    "print(\"Standard Deviations:\", svm_std_scores_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26517e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7221\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7361\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6687\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7184\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7361\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6815\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7295\n",
      "Fold 6: 0.6836\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6787\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.6663\n",
      "Fold 6: 0.6774\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7308\n",
      "Fold 10: 0.7295\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.6663\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.6600\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7258\n",
      "Fold 8: 0.7308\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.6729\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Mean Scores for SVM: [0.70209642 0.70209688 0.70209565 0.70209827 0.70209442 0.70209411\n",
      " 0.70209657 0.70209565 0.70209442 0.70210165]\n",
      "Standard Deviations for SVM: [0.0133019  0.01303167 0.01946782 0.01296401 0.0190638  0.01985938\n",
      " 0.01208121 0.0171276  0.0189354  0.01497633]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "654f2d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020964052136853\n",
      "Standard Deviations: 0.01635176017599463\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_26 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_26 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_26)\n",
    "print(\"Standard Deviations:\", svm_std_scores_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c279ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7370\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7311\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.6538\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.6691\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7345\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7171\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7295\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.6687\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7249\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6799\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7295\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Mean Scores for SVM: [0.70209319 0.7020935  0.70209242 0.70209734 0.70209258 0.70209335\n",
      " 0.70209734 0.70209458 0.70209181 0.70209319]\n",
      "Standard Deviations for SVM: [0.01607061 0.01142956 0.01324891 0.00841554 0.01402689 0.01732151\n",
      " 0.01697523 0.01425822 0.01540038 0.01705404]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59eb3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020939299737718\n",
      "Standard Deviations: 0.014668025062923234\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_27 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_27 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_27)\n",
    "print(\"Standard Deviations:\", svm_std_scores_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29ad5dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.6849\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.7308\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6725\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.6811\n",
      "Fold 9: 0.7270\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.6774\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7249\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6667\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7283\n",
      "Fold 5: 0.7295\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.6774\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Mean Scores for SVM: [0.70209673 0.7020955  0.70209519 0.70209442 0.70209534 0.70209119\n",
      " 0.70209396 0.70209411 0.70210226 0.7020935 ]\n",
      "Standard Deviations for SVM: [0.01530605 0.01413035 0.00792904 0.01234487 0.01304516 0.01489389\n",
      " 0.00901741 0.00943524 0.02093969 0.01160286]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd79eaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020952214032918\n",
      "Standard Deviations: 0.013359064167477782\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_28 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_28 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_28)\n",
    "print(\"Standard Deviations:\", svm_std_scores_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eedb4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.7261\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.6737\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7295\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.6787\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6725\n",
      "Fold 6: 0.7246\n",
      "Fold 7: 0.7221\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7571\n",
      "Fold 3: 0.6704\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.7333\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6803\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7295\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7233\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7212\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6799\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7196\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.7212\n",
      "Fold 3: 0.7311\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6799\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.6849\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6654\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6787\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Mean Scores for SVM: [0.70209596 0.70209181 0.70209657 0.70209657 0.70209196 0.70210073\n",
      " 0.70209504 0.70209073 0.70209842 0.70209642]\n",
      "Standard Deviations for SVM: [0.01066079 0.0156761  0.01391983 0.01544126 0.02425041 0.01426538\n",
      " 0.01373974 0.01560421 0.01607932 0.01389652]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a4fd858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020954212673842\n",
      "Standard Deviations: 0.01570700664592841\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_29 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_29 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_29)\n",
    "print(\"Standard Deviations:\", svm_std_scores_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "344c4a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.7208\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6774\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7283\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7320\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6712\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6815\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7258\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6811\n",
      "Fold 7: 0.7221\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7274\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7311\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6836\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7134\n",
      "Fold 10: 0.7270\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.6700\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7320\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Mean Scores for SVM: [0.7020955  0.70209335 0.70209488 0.70209857 0.70209381 0.70209688\n",
      " 0.70209488 0.70209534 0.7020975  0.70209411]\n",
      "Standard Deviations for SVM: [0.01471322 0.01274569 0.01577963 0.01401879 0.01376793 0.01461683\n",
      " 0.01307405 0.01672739 0.01589399 0.01312824]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ed30e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020954827640282\n",
      "Standard Deviations: 0.01450299169453152\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_30 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_30 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_30)\n",
    "print(\"Standard Deviations:\", svm_std_scores_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "180e7760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6555\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6716\n",
      "Fold 4: 0.7407\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7345\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.7345\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7196\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6654\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7283\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.6592\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7320\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6629\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7261\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.6749\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6737\n",
      "Fold 10: 0.7122\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6778\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7286\n",
      "Fold 4: 0.6787\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7159\n",
      "Fold 7: 0.6762\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6774\n",
      "Fold 6: 0.6774\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7295\n",
      "\n",
      "Mean Scores for SVM: [0.70210411 0.7020998  0.70209903 0.70210134 0.70209611 0.7020978\n",
      " 0.70209519 0.70209673 0.70209442 0.70209488]\n",
      "Standard Deviations for SVM: [0.02742119 0.01374948 0.01947983 0.0176145  0.00985451 0.0158374\n",
      " 0.01689936 0.01866153 0.01126703 0.01681555]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "45278e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702097942629781\n",
      "Standard Deviations: 0.017384443744305294\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_31 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_31 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_31)\n",
    "print(\"Standard Deviations:\", svm_std_scores_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b08f6500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6815\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7274\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7333\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7212\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6787\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6712\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7233\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7361\n",
      "Fold 2: 0.7373\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.6725\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.6663\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6923\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.6849\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.6737\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7246\n",
      "Fold 7: 0.6762\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7212\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Mean Scores for SVM: [0.70209704 0.70209473 0.70209842 0.70209842 0.70209335 0.70208935\n",
      " 0.70209058 0.70209396 0.70209411 0.70209227]\n",
      "Standard Deviations for SVM: [0.01671666 0.01144969 0.01527268 0.01333712 0.01728601 0.02279949\n",
      " 0.01363738 0.01197212 0.01460603 0.01035875]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3cd945a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020942220828297\n",
      "Standard Deviations: 0.015133566918578204\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_32 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_32 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_32)\n",
    "print(\"Standard Deviations:\", svm_std_scores_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1d2ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.7283\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.7258\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6704\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7270\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.6654\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7283\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7175\n",
      "Fold 2: 0.7423\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6538\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7208\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.7382\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7308\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Mean Scores for SVM: [0.70209458 0.70209765 0.70209288 0.70210088 0.70209842 0.7020915\n",
      " 0.70209519 0.70209734 0.7020978  0.70209842]\n",
      "Standard Deviations for SVM: [0.00945277 0.016187   0.01446889 0.01619645 0.01800959 0.02344631\n",
      " 0.0094618  0.01520478 0.01086004 0.01203782]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3541ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020964667103293\n",
      "Standard Deviations: 0.015101931369858023\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_33 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_33 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_33)\n",
    "print(\"Standard Deviations:\", svm_std_scores_33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc882c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7258\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6799\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7345\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7162\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7146\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6774\n",
      "Fold 10: 0.7382\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7233\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.6836\n",
      "Fold 9: 0.7270\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.6741\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7208\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7261\n",
      "Fold 4: 0.6712\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.6762\n",
      "Fold 8: 0.7494\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6687\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6667\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.6787\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Mean Scores for SVM: [0.70209873 0.70209657 0.70209796 0.70209442 0.70209919 0.70209565\n",
      " 0.70209857 0.70209104 0.70210011 0.70209381]\n",
      "Standard Deviations for SVM: [0.01224625 0.01243518 0.0132797  0.01314474 0.01745133 0.01452396\n",
      " 0.01404789 0.02470472 0.01602987 0.01008827]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "937ce7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020966050777777\n",
      "Standard Deviations: 0.015282767908083068\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_34 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_34 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_34)\n",
    "print(\"Standard Deviations:\", svm_std_scores_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9264535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6791\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7432\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.7295\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7246\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.7295\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7385\n",
      "Fold 2: 0.7212\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6687\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.7122\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6687\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.6629\n",
      "Fold 4: 0.7407\n",
      "Fold 5: 0.6712\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7299\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7333\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.6787\n",
      "Fold 10: 0.6811\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7385\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.6749\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6774\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.7295\n",
      "Fold 8: 0.6762\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6787\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6753\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.7233\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.6811\n",
      "\n",
      "Mean Scores for SVM: [0.7020995  0.70209719 0.70209903 0.70208873 0.70209796 0.70209212\n",
      " 0.70209196 0.70209488 0.70209242 0.70209842]\n",
      "Standard Deviations for SVM: [0.01832153 0.01612676 0.01548258 0.02078601 0.02188963 0.01831224\n",
      " 0.01623749 0.01117275 0.01602676 0.01694829]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8c94c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702095221403292\n",
      "Standard Deviations: 0.017362607284196765\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_35 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_35 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_35)\n",
    "print(\"Standard Deviations:\", svm_std_scores_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d9ce93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.6642\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6766\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7221\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.6762\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6753\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6865\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7233\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6803\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7221\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6787\n",
      "Fold 10: 0.7184\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7274\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7283\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.6650\n",
      "Fold 7: 0.6749\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.6753\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7357\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6716\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.7171\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.7146\n",
      "Fold 10: 0.6663\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Mean Scores for SVM: [0.70210026 0.70209811 0.70209719 0.70210149 0.70210196 0.70209427\n",
      " 0.70209319 0.70209919 0.70209704 0.7020955 ]\n",
      "Standard Deviations for SVM: [0.0151261  0.00564503 0.01602952 0.01308651 0.01575724 0.02068788\n",
      " 0.0088934  0.0165035  0.01874014 0.01102842]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b041bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020978196364935\n",
      "Standard Deviations: 0.014796700140899746\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_36 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_36 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_36)\n",
    "print(\"Standard Deviations:\", svm_std_scores_36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e983001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6691\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.6787\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.6729\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7224\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.6914\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7184\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7258\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.7100\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7208\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7261\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.6667\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7270\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.7109\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7283\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.7159\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7113\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7457\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7249\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.7308\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Mean Scores for SVM: [0.70209611 0.70209827 0.7020935  0.70209657 0.7020995  0.70209458\n",
      " 0.70209519 0.70209888 0.70209734 0.70209396]\n",
      "Standard Deviations for SVM: [0.01676713 0.01463314 0.01057291 0.01141408 0.0120369  0.00929076\n",
      " 0.01810333 0.01216799 0.017403   0.01466109]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ef1c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020963898395245\n",
      "Standard Deviations: 0.014007642348296524\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_37 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_37 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_37)\n",
    "print(\"Standard Deviations:\", svm_std_scores_37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d561f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.6704\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.6700\n",
      "Fold 9: 0.7233\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7100\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.6824\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7395\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6791\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7361\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6762\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7171\n",
      "Fold 8: 0.6625\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6741\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7370\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7196\n",
      "Fold 10: 0.7184\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.6787\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.6762\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7026\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.6642\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.6799\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7249\n",
      "Fold 2: 0.6642\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Mean Scores for SVM: [0.70209627 0.7020978  0.70209411 0.70210257 0.70210026 0.70209519\n",
      " 0.70209396 0.70209565 0.70209596 0.70209873]\n",
      "Standard Deviations for SVM: [0.01837708 0.01654213 0.02141035 0.01382698 0.01653409 0.01105651\n",
      " 0.01228861 0.0101721  0.01730773 0.01883892]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6321c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020970509284454\n",
      "Standard Deviations: 0.016017223056141963\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_38 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_38 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_38)\n",
    "print(\"Standard Deviations:\", svm_std_scores_38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1631d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.7184\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7249\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.7274\n",
      "Fold 4: 0.7333\n",
      "Fold 5: 0.6737\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6700\n",
      "Fold 8: 0.6725\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.7320\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7060\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7333\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7295\n",
      "Fold 9: 0.6762\n",
      "Fold 10: 0.6849\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6803\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6836\n",
      "Fold 6: 0.7444\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.6774\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7249\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Mean Scores for SVM: [0.70209581 0.7020875  0.70209288 0.70209704 0.70209704 0.70209688\n",
      " 0.70210011 0.70209657 0.70209165 0.70209411]\n",
      "Standard Deviations for SVM: [0.01307694 0.02311751 0.00934004 0.01361965 0.01503148 0.01855105\n",
      " 0.01686803 0.01484021 0.0129733  0.01143596]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56356874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020949600425559\n",
      "Standard Deviations: 0.015336536281175813\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_39 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_39 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_39)\n",
    "print(\"Standard Deviations:\", svm_std_scores_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f5bb6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.6803\n",
      "Fold 4: 0.6911\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7246\n",
      "Fold 8: 0.7382\n",
      "Fold 9: 0.6787\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.6679\n",
      "Fold 3: 0.7249\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.7221\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.7261\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7233\n",
      "Fold 5: 0.6774\n",
      "Fold 6: 0.6923\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6948\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.6861\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7134\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.6762\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7221\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7134\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7184\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6973\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6654\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6787\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.7295\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.6828\n",
      "Fold 3: 0.6729\n",
      "Fold 4: 0.7382\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6836\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.7283\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Mean Scores for SVM: [0.7020975  0.7020995  0.70209181 0.70209596 0.70209427 0.70209719\n",
      " 0.7021018  0.70209857 0.70209857 0.70210088]\n",
      "Standard Deviations for SVM: [0.01859471 0.01768006 0.01490139 0.0101307  0.01135523 0.01268056\n",
      " 0.01297458 0.01743592 0.0108953  0.019899  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "65da5c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020976043982398\n",
      "Standard Deviations: 0.015032622417216987\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_40 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_40 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_40)\n",
    "print(\"Standard Deviations:\", svm_std_scores_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54f66ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7345\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7109\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6753\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.6799\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7333\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.7258\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7233\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.6663\n",
      "Fold 10: 0.6824\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.6753\n",
      "Fold 4: 0.7270\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.7308\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.7060\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6778\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6729\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.6778\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.6998\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.6737\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6873\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Mean Scores for SVM: [0.70209827 0.70209473 0.70209811 0.70209581 0.7020995  0.70209704\n",
      " 0.70209796 0.70209335 0.70209888 0.70209258]\n",
      "Standard Deviations for SVM: [0.01492682 0.01353117 0.0167283  0.01759813 0.01660219 0.01008055\n",
      " 0.01355689 0.01105104 0.01218885 0.01377196]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "030117d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020966204519388\n",
      "Standard Deviations: 0.01420152320104084\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_41 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_41 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_41)\n",
    "print(\"Standard Deviations:\", svm_std_scores_41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9515c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7444\n",
      "Fold 8: 0.7184\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6766\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7072\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7208\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6815\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.7382\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.6886\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.6849\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7274\n",
      "Fold 2: 0.7299\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.6712\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6886\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7233\n",
      "Fold 6: 0.7246\n",
      "Fold 7: 0.6737\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7237\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7246\n",
      "Fold 6: 0.6836\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6716\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7345\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7361\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.6774\n",
      "Fold 5: 0.6712\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7357\n",
      "Fold 5: 0.7320\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.6625\n",
      "Fold 8: 0.6960\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Mean Scores for SVM: [0.70209857 0.70209996 0.70209719 0.70209042 0.70209365 0.70209888\n",
      " 0.70209442 0.70210026 0.70209089 0.70209565]\n",
      "Standard Deviations for SVM: [0.0173999  0.01286337 0.01700249 0.0179563  0.01377649 0.01495918\n",
      " 0.01444057 0.01743586 0.02128168 0.02095465]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ea436c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020959901113397\n",
      "Standard Deviations: 0.017023078903212644\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_42 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_42 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_42)\n",
    "print(\"Standard Deviations:\", svm_std_scores_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cef734b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.7072\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.7125\n",
      "Fold 3: 0.7187\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.6576\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.6939\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.6886\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7270\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6911\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.6973\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.7097\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6898\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.7051\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7171\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6663\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6725\n",
      "Fold 5: 0.7283\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7208\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.7435\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6774\n",
      "Fold 5: 0.6737\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.6890\n",
      "Fold 3: 0.6815\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7246\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.7237\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.6935\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6700\n",
      "Fold 10: 0.7308\n",
      "\n",
      "Mean Scores for SVM: [0.70209704 0.70209504 0.70209488 0.70209181 0.70209534 0.70209519\n",
      " 0.70209888 0.70209258 0.70210149 0.70209473]\n",
      "Standard Deviations for SVM: [0.01107395 0.01946705 0.01143342 0.01080478 0.00848551 0.01542491\n",
      " 0.01568229 0.0204373  0.01348286 0.01794106]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98202e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020956980022816\n",
      "Standard Deviations: 0.014920734395508173\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_43 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_43 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_43)\n",
    "print(\"Standard Deviations:\", svm_std_scores_43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb02ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7001\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.6898\n",
      "Fold 6: 0.6886\n",
      "Fold 7: 0.6749\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7270\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7270\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6964\n",
      "Fold 2: 0.7299\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7184\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.6836\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7283\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6828\n",
      "Fold 2: 0.6902\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.7072\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.7221\n",
      "Fold 9: 0.7084\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7001\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.7274\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6774\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.7283\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6799\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.7196\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7113\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.6778\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6985\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7146\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.6803\n",
      "Fold 4: 0.6687\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6861\n",
      "Fold 7: 0.7258\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7395\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6914\n",
      "Fold 2: 0.6791\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.7035\n",
      "\n",
      "Mean Scores for SVM: [0.70209488 0.70209581 0.70209411 0.70209811 0.70209534 0.70209165\n",
      " 0.70209857 0.70209827 0.70209857 0.70197235]\n",
      "Standard Deviations for SVM: [0.0163335  0.01522152 0.01756941 0.01159936 0.01108694 0.017664\n",
      " 0.01299364 0.01168829 0.02020562 0.01499099]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f3e6a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020837676533802\n",
      "Standard Deviations: 0.015215473158646397\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_44 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_44 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_44)\n",
    "print(\"Standard Deviations:\", svm_std_scores_44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "918bf68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.7320\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7022\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.7146\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.6824\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.7357\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7336\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7162\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6638\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.7162\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.7233\n",
      "Fold 9: 0.7047\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7051\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.7323\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.6911\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.7208\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.6815\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7109\n",
      "Fold 7: 0.7022\n",
      "Fold 8: 0.7246\n",
      "Fold 9: 0.7308\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7014\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.7159\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.6873\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7038\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7333\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.7010\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7063\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6725\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.7171\n",
      "Fold 8: 0.7270\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7010\n",
      "Fold 6: 0.7084\n",
      "Fold 7: 0.7481\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Mean Scores for SVM: [0.70209765 0.70209611 0.70209027 0.70209519 0.70209227 0.70210149\n",
      " 0.7020975  0.70209688 0.70209442 0.70210011]\n",
      "Standard Deviations for SVM: [0.01118618 0.01515778 0.01857594 0.01224427 0.01490043 0.01583508\n",
      " 0.00966872 0.01260328 0.01450699 0.01766297]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "99d1b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702096189975432\n",
      "Standard Deviations: 0.014482437481243164\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_45 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_45 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_45)\n",
    "print(\"Standard Deviations:\", svm_std_scores_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f77c97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7419\n",
      "Fold 6: 0.7333\n",
      "Fold 7: 0.6849\n",
      "Fold 8: 0.6675\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.7286\n",
      "Fold 2: 0.6704\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7109\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6960\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.7196\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.7196\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7410\n",
      "Fold 2: 0.6952\n",
      "Fold 3: 0.7261\n",
      "Fold 4: 0.7022\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.6613\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6762\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7249\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7246\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.6712\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.6840\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.6787\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7072\n",
      "Fold 8: 0.7010\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6976\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7175\n",
      "Fold 4: 0.7246\n",
      "Fold 5: 0.6973\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.6886\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.6638\n",
      "Fold 10: 0.6935\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7076\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.6973\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7200\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6948\n",
      "Fold 5: 0.6886\n",
      "Fold 6: 0.7122\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7122\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7175\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6935\n",
      "Fold 6: 0.6873\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7258\n",
      "Fold 10: 0.6762\n",
      "\n",
      "Mean Scores for SVM: [0.70209765 0.70209657 0.70209673 0.70208904 0.70209288 0.70209504\n",
      " 0.70209442 0.7020975  0.70209381 0.70209411]\n",
      "Standard Deviations for SVM: [0.02083774 0.01520753 0.01349048 0.02234941 0.01584314 0.01369302\n",
      " 0.01735682 0.01156772 0.01013447 0.0137609 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "677e4dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020947755526242\n",
      "Standard Deviations: 0.015851229087221773\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_46 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_46 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_46)\n",
    "print(\"Standard Deviations:\", svm_std_scores_46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30bdbdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6828\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6911\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.7097\n",
      "Fold 10: 0.7233\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6568\n",
      "Fold 2: 0.7187\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7208\n",
      "Fold 5: 0.7022\n",
      "Fold 6: 0.6799\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7221\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.6791\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.7072\n",
      "Fold 6: 0.7221\n",
      "Fold 7: 0.6898\n",
      "Fold 8: 0.6935\n",
      "Fold 9: 0.7171\n",
      "Fold 10: 0.6923\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.6877\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.6923\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.7047\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7246\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6890\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.7184\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.7283\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.6861\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7224\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.6873\n",
      "Fold 5: 0.7047\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.6948\n",
      "Fold 10: 0.7233\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7286\n",
      "Fold 2: 0.7249\n",
      "Fold 3: 0.6815\n",
      "Fold 4: 0.7122\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.6774\n",
      "Fold 8: 0.6898\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7047\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7014\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.6824\n",
      "Fold 10: 0.7084\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6914\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.7146\n",
      "Fold 5: 0.6811\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7047\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.6985\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.6617\n",
      "Fold 3: 0.7299\n",
      "Fold 4: 0.6960\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7010\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.7035\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Mean Scores for SVM: [0.70210011 0.70209873 0.70209842 0.70209734 0.7020995  0.70209458\n",
      " 0.70209242 0.70209504 0.7020978  0.70209611]\n",
      "Standard Deviations for SVM: [0.01192991 0.0193126  0.01279243 0.01309228 0.01377882 0.01201619\n",
      " 0.01603617 0.01167206 0.01193833 0.0167694 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aaed5959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020970048059627\n",
      "Standard Deviations: 0.014148250366235824\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_47 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_47 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_47)\n",
    "print(\"Standard Deviations:\", svm_std_scores_47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c9e94ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.7249\n",
      "Fold 4: 0.7060\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7060\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6865\n",
      "Fold 2: 0.6927\n",
      "Fold 3: 0.7212\n",
      "Fold 4: 0.7047\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7184\n",
      "Fold 7: 0.7010\n",
      "Fold 8: 0.7196\n",
      "Fold 9: 0.6836\n",
      "Fold 10: 0.6973\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7311\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.6861\n",
      "Fold 5: 0.6836\n",
      "Fold 6: 0.6960\n",
      "Fold 7: 0.6935\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7246\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.6840\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7146\n",
      "Fold 9: 0.6998\n",
      "Fold 10: 0.7171\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7076\n",
      "Fold 2: 0.6753\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.7246\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.6998\n",
      "Fold 8: 0.7246\n",
      "Fold 9: 0.6923\n",
      "Fold 10: 0.7022\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7088\n",
      "Fold 3: 0.7113\n",
      "Fold 4: 0.7258\n",
      "Fold 5: 0.6873\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.6687\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6939\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.7171\n",
      "Fold 6: 0.7333\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.6787\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.6886\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7200\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6989\n",
      "Fold 4: 0.6824\n",
      "Fold 5: 0.6799\n",
      "Fold 6: 0.6824\n",
      "Fold 7: 0.7208\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.7051\n",
      "Fold 4: 0.7159\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.7134\n",
      "Fold 7: 0.7295\n",
      "Fold 8: 0.6998\n",
      "Fold 9: 0.6811\n",
      "Fold 10: 0.6774\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7237\n",
      "Fold 2: 0.7138\n",
      "Fold 3: 0.6877\n",
      "Fold 4: 0.7084\n",
      "Fold 5: 0.6985\n",
      "Fold 6: 0.6911\n",
      "Fold 7: 0.6824\n",
      "Fold 8: 0.6861\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Mean Scores for SVM: [0.70209519 0.70209673 0.70209042 0.70209796 0.7020998  0.70209196\n",
      " 0.70209796 0.70209273 0.70209581 0.70209365]\n",
      "Standard Deviations for SVM: [0.01181696 0.01294994 0.01553314 0.01220128 0.01400011 0.01878489\n",
      " 0.01571054 0.01478099 0.01499139 0.01394434]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16317898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.702095221403292\n",
      "Standard Deviations: 0.014597437884183889\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_48 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_48 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_48)\n",
    "print(\"Standard Deviations:\", svm_std_scores_48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a353fa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.7187\n",
      "Fold 2: 0.7435\n",
      "Fold 3: 0.6853\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.6824\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7208\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.6853\n",
      "Fold 3: 0.7026\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.6849\n",
      "Fold 7: 0.6973\n",
      "Fold 8: 0.7072\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7224\n",
      "Fold 2: 0.6976\n",
      "Fold 3: 0.6964\n",
      "Fold 4: 0.6811\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7047\n",
      "Fold 8: 0.7084\n",
      "Fold 9: 0.7010\n",
      "Fold 10: 0.7109\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.7150\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7295\n",
      "Fold 5: 0.6749\n",
      "Fold 6: 0.6935\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.6948\n",
      "Fold 9: 0.7221\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.6840\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.6778\n",
      "Fold 4: 0.7097\n",
      "Fold 5: 0.7208\n",
      "Fold 6: 0.7035\n",
      "Fold 7: 0.6861\n",
      "Fold 8: 0.7159\n",
      "Fold 9: 0.7122\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7175\n",
      "Fold 2: 0.7063\n",
      "Fold 3: 0.6914\n",
      "Fold 4: 0.6849\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.7097\n",
      "Fold 7: 0.6960\n",
      "Fold 8: 0.7171\n",
      "Fold 9: 0.6749\n",
      "Fold 10: 0.7146\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.7026\n",
      "Fold 3: 0.7063\n",
      "Fold 4: 0.7035\n",
      "Fold 5: 0.7270\n",
      "Fold 6: 0.7382\n",
      "Fold 7: 0.7060\n",
      "Fold 8: 0.6923\n",
      "Fold 9: 0.6749\n",
      "Fold 10: 0.6576\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7299\n",
      "Fold 3: 0.7150\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6687\n",
      "Fold 6: 0.6985\n",
      "Fold 7: 0.7171\n",
      "Fold 8: 0.6873\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.7097\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7088\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.7200\n",
      "Fold 4: 0.7010\n",
      "Fold 5: 0.7060\n",
      "Fold 6: 0.6663\n",
      "Fold 7: 0.7035\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.7134\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.7125\n",
      "Fold 2: 0.6691\n",
      "Fold 3: 0.6952\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7122\n",
      "Fold 6: 0.7196\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.6973\n",
      "Fold 9: 0.7159\n",
      "Fold 10: 0.7072\n",
      "\n",
      "Mean Scores for SVM: [0.70209089 0.70209888 0.70209473 0.70209519 0.70210134 0.70209488\n",
      " 0.70209411 0.70208996 0.70209319 0.70209965]\n",
      "Standard Deviations for SVM: [0.0186581  0.01135552 0.01024279 0.01591839 0.0139003  0.01385543\n",
      " 0.02210559 0.01701685 0.0134298  0.01410929]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5967066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020952828999357\n",
      "Standard Deviations: 0.015421529210724115\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_49 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_49 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_49)\n",
    "print(\"Standard Deviations:\", svm_std_scores_49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "70dfdd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Fold 1: 0.6853\n",
      "Fold 2: 0.6865\n",
      "Fold 3: 0.7299\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.6998\n",
      "Fold 6: 0.6749\n",
      "Fold 7: 0.7196\n",
      "Fold 8: 0.6985\n",
      "Fold 9: 0.7109\n",
      "Fold 10: 0.7258\n",
      "\n",
      "Iteration 2:\n",
      "Fold 1: 0.6952\n",
      "Fold 2: 0.6964\n",
      "Fold 3: 0.7311\n",
      "Fold 4: 0.7295\n",
      "Fold 5: 0.6960\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7146\n",
      "Fold 8: 0.6787\n",
      "Fold 9: 0.6849\n",
      "Fold 10: 0.6998\n",
      "\n",
      "Iteration 3:\n",
      "Fold 1: 0.7038\n",
      "Fold 2: 0.6989\n",
      "Fold 3: 0.6890\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6514\n",
      "Fold 6: 0.6898\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.7457\n",
      "Fold 9: 0.7432\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 4:\n",
      "Fold 1: 0.6902\n",
      "Fold 2: 0.7150\n",
      "Fold 3: 0.6976\n",
      "Fold 4: 0.7233\n",
      "Fold 5: 0.6911\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.6700\n",
      "Fold 8: 0.7270\n",
      "Fold 9: 0.6985\n",
      "Fold 10: 0.7134\n",
      "\n",
      "Iteration 5:\n",
      "Fold 1: 0.7138\n",
      "Fold 2: 0.7038\n",
      "Fold 3: 0.6902\n",
      "Fold 4: 0.6836\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7159\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.7184\n",
      "Fold 10: 0.6799\n",
      "\n",
      "Iteration 6:\n",
      "Fold 1: 0.7398\n",
      "Fold 2: 0.7001\n",
      "Fold 3: 0.7125\n",
      "Fold 4: 0.6898\n",
      "Fold 5: 0.7035\n",
      "Fold 6: 0.6948\n",
      "Fold 7: 0.7084\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.6861\n",
      "Fold 10: 0.6836\n",
      "\n",
      "Iteration 7:\n",
      "Fold 1: 0.6877\n",
      "Fold 2: 0.7014\n",
      "Fold 3: 0.7088\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.7022\n",
      "Fold 7: 0.7109\n",
      "Fold 8: 0.7134\n",
      "Fold 9: 0.7060\n",
      "Fold 10: 0.6960\n",
      "\n",
      "Iteration 8:\n",
      "Fold 1: 0.6989\n",
      "Fold 2: 0.6791\n",
      "Fold 3: 0.6927\n",
      "Fold 4: 0.6998\n",
      "Fold 5: 0.7097\n",
      "Fold 6: 0.7370\n",
      "Fold 7: 0.7134\n",
      "Fold 8: 0.7097\n",
      "Fold 9: 0.6935\n",
      "Fold 10: 0.6873\n",
      "\n",
      "Iteration 9:\n",
      "Fold 1: 0.7100\n",
      "Fold 2: 0.7076\n",
      "Fold 3: 0.6939\n",
      "Fold 4: 0.7134\n",
      "Fold 5: 0.7084\n",
      "Fold 6: 0.6774\n",
      "Fold 7: 0.7221\n",
      "Fold 8: 0.7022\n",
      "Fold 9: 0.6911\n",
      "Fold 10: 0.6948\n",
      "\n",
      "Iteration 10:\n",
      "Fold 1: 0.6927\n",
      "Fold 2: 0.7249\n",
      "Fold 3: 0.7138\n",
      "Fold 4: 0.6985\n",
      "Fold 5: 0.6948\n",
      "Fold 6: 0.6836\n",
      "Fold 7: 0.6923\n",
      "Fold 8: 0.7035\n",
      "Fold 9: 0.6898\n",
      "Fold 10: 0.7270\n",
      "\n",
      "Mean Scores for SVM: [0.70209657 0.70209396 0.7020978  0.70209642 0.70209581 0.70209027\n",
      " 0.70209704 0.70210042 0.70209534 0.70209288]\n",
      "Standard Deviations for SVM: [0.01775135 0.01665402 0.02617866 0.01657724 0.01275982 0.01541221\n",
      " 0.00753635 0.01544281 0.01229274 0.01422349]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0):\n",
    "        self.model = LinearSVC(C=C, max_iter=10000)  # Increase max_iter if convergence warning appears\n",
    "        self.C = C\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVMClassifier()\n",
    "\n",
    "# For demonstration purposes, I've reduced these numbers\n",
    "n_iterations = 10  # Number of iterations\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Initialize a list to store cross-validation scores for SVM\n",
    "svm_cv_scores = []\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Perform cross-validation and calculate scores for SVM\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "    scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Append the scores to the list for SVM\n",
    "    svm_cv_scores.append(scores)\n",
    "\n",
    "# Convert the list of scores for SVM to a NumPy array for analysis\n",
    "svm_cv_scores = np.array(svm_cv_scores)\n",
    "\n",
    "# Print the cross-validation scores for each iteration and fold for SVM\n",
    "for i, scores in enumerate(svm_cv_scores):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    for j, score in enumerate(scores):\n",
    "        print(f\"Fold {j + 1}: {score:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores for SVM\n",
    "svm_mean_scores = np.mean(svm_cv_scores, axis=1)\n",
    "svm_std_scores = np.std(svm_cv_scores, axis=1)\n",
    "print(\"Mean Scores for SVM:\", svm_mean_scores)\n",
    "print(\"Standard Deviations for SVM:\", svm_std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70ebe381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Scores: 0.7020956518797986\n",
      "Standard Deviations: 0.016129597433592064\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean and standard deviation of the scores\n",
    "svm_mean_scores_50 = np.mean(svm_cv_scores)\n",
    "svm_std_scores_50 = np.std(svm_cv_scores)\n",
    "print(\"Mean Scores:\", svm_mean_scores_50)\n",
    "print(\"Standard Deviations:\", svm_std_scores_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4cf98725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7020973737858259, 0.7020835524151269, 0.7020964820844902, 0.7020966819485828, 0.702096881812675, 0.7020963283428806, 0.702097204670055, 0.7020970663026066, 0.7020960362338226, 0.7020967434452264, 0.7020959747371787, 0.7020970048059628, 0.702095851743891, 0.702096174601271, 0.702096282220398, 0.7020962668462368, 0.7020973891599867, 0.70209728154086, 0.7020950984100045, 0.702096574329456, 0.7020966973227438, 0.7020955135123502, 0.702098296235483, 0.7020946218110146, 0.7020957287506034, 0.7020964052136853, 0.7020939299737718, 0.7020952214032918, 0.7020954212673842, 0.7020954827640282, 0.702097942629781, 0.7020942220828297, 0.7020964667103293, 0.7020966050777777, 0.702095221403292, 0.7020978196364935, 0.7020963898395245, 0.7020970509284454, 0.7020949600425559, 0.7020976043982398, 0.7020966204519388, 0.7020959901113397, 0.7020956980022816, 0.7020837676533802, 0.702096189975432, 0.7020947755526242, 0.7020970048059627, 0.702095221403292, 0.7020952828999357, 0.7020956518797986]\n"
     ]
    }
   ],
   "source": [
    "values1 = [svm_mean_scores_1,svm_mean_scores_2,svm_mean_scores_3,svm_mean_scores_4,svm_mean_scores_5,svm_mean_scores_6,svm_mean_scores_7,svm_mean_scores_8,svm_mean_scores_9,svm_mean_scores_10,svm_mean_scores_11,svm_mean_scores_12,svm_mean_scores_13,svm_mean_scores_14,svm_mean_scores_15,svm_mean_scores_16,svm_mean_scores_17,svm_mean_scores_18,svm_mean_scores_19,svm_mean_scores_20,svm_mean_scores_21,svm_mean_scores_22,svm_mean_scores_23,svm_mean_scores_24,svm_mean_scores_25,svm_mean_scores_26,svm_mean_scores_27,svm_mean_scores_28,svm_mean_scores_29,svm_mean_scores_30,svm_mean_scores_31,svm_mean_scores_32,svm_mean_scores_33,svm_mean_scores_34,svm_mean_scores_35,svm_mean_scores_36,svm_mean_scores_37,svm_mean_scores_38,svm_mean_scores_39,svm_mean_scores_40,svm_mean_scores_41,svm_mean_scores_42,svm_mean_scores_43,svm_mean_scores_44,svm_mean_scores_45,svm_mean_scores_46,svm_mean_scores_47,svm_mean_scores_48,svm_mean_scores_49,svm_mean_scores_50]\n",
    "print(values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "222d6e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016514117685305956, 0.01441683325813612, 0.014857393211007831, 0.015630643495323483, 0.014774934430435633, 0.01487510855727549, 0.014381126483736727, 0.014555153071886128, 0.016756105107580563, 0.013815538747284516, 0.01524364906970056, 0.01633295858813219, 0.014766078491622746, 0.015159145006040304, 0.01608384222842572, 0.017649937647198417, 0.01368327493932222, 0.015226085112575035, 0.017607940399214837, 0.016811362611735135, 0.01582795170278491, 0.01624474241111219, 0.016558159170189757, 0.014732664365182642, 0.014500840058817395, 0.01635176017599463, 0.014668025062923234, 0.013359064167477782, 0.01570700664592841, 0.01450299169453152, 0.017384443744305294, 0.015133566918578204, 0.015101931369858023, 0.015282767908083068, 0.017362607284196765, 0.014796700140899746, 0.014007642348296524, 0.016017223056141963, 0.015336536281175813, 0.015032622417216987, 0.01420152320104084, 0.017023078903212644, 0.014920734395508173, 0.015215473158646397, 0.014482437481243164, 0.015851229087221773, 0.014148250366235824, 0.014597437884183889, 0.015421529210724115, 0.016129597433592064]\n"
     ]
    }
   ],
   "source": [
    "values2 = [svm_std_scores_1,svm_std_scores_2,svm_std_scores_3,svm_std_scores_4,svm_std_scores_5,svm_std_scores_6,svm_std_scores_7,svm_std_scores_8,svm_std_scores_9,svm_std_scores_10,svm_std_scores_11,svm_std_scores_12,svm_std_scores_13,svm_std_scores_14,svm_std_scores_15,svm_std_scores_16,svm_std_scores_17,svm_std_scores_18,svm_std_scores_19,svm_std_scores_20,svm_std_scores_21,svm_std_scores_22,svm_std_scores_23,svm_std_scores_24,svm_std_scores_25,svm_std_scores_26,svm_std_scores_27,svm_std_scores_28,svm_std_scores_29,svm_std_scores_30,svm_std_scores_31,svm_std_scores_32,svm_std_scores_33,svm_std_scores_34,svm_std_scores_35,svm_std_scores_36,svm_std_scores_37,svm_std_scores_38,svm_std_scores_39,svm_std_scores_40,svm_std_scores_41,svm_std_scores_42,svm_std_scores_43,svm_std_scores_44,svm_std_scores_45,svm_std_scores_46,svm_std_scores_47,svm_std_scores_48,svm_std_scores_49,svm_std_scores_50]\n",
    "print(values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e028a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702095721063523\n",
      "0.015380835323744868\n"
     ]
    }
   ],
   "source": [
    "mean=np.mean(values1)\n",
    "std=np.mean(values2)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6939cdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generating x-values based on the index of the list\n",
    "x_values1 = list(range(len(values1)))\n",
    "\n",
    "# Scatter plot with lines connecting the points\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_values1, values1, color='blue', marker='o', s=100)  # s is the marker size\n",
    "plt.plot(x_values1, values1, color='blue', linestyle='-', linewidth=1.5)  # Connecting the points with a line\n",
    "\n",
    "# Providing labels for the axes and the title for the graph\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Scatter Plot with Connected Points')\n",
    "plt.xticks(x_values1)  # Setting x-ticks to match the indices\n",
    "\n",
    "# Displaying the plot\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5e10cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generating x-values based on the index of the list\n",
    "x_values2 = list(range(len(values2)))\n",
    "\n",
    "# Scatter plot with lines connecting the points\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_values2, values2, color='blue', marker='o', s=100)  # s is the marker size\n",
    "plt.plot(x_values2, values2, color='blue', linestyle='-', linewidth=1.5)  # Connecting the points with a line\n",
    "\n",
    "# Providing labels for the axes and the title for the graph\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Scatter Plot with Connected Points')\n",
    "plt.xticks(x_values2)  # Setting x-ticks to match the indices\n",
    "\n",
    "# Displaying the plot\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33233755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10010904479394055\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X = X_test[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "27208e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.0999999999999979\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "X_test1 = scaler.transform(X_test1)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X1 = X_test1[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X1)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1b6fc200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10000000000000356\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "X_test2 = scaler.transform(X_test2)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train2, y_train2)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X2 = X_test2[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X2)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "65a9db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10000000000000045\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train3 = scaler.fit_transform(X_train3)\n",
    "X_test3 = scaler.transform(X_test3)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X3 = X_test3[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X3)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8e57852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10000000000000334\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train4 = scaler.fit_transform(X_train4)\n",
    "X_test4 = scaler.transform(X_test4)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X4 = X_test4[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X4)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "72a61cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10000000000000067\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train5 = scaler.fit_transform(X_train5)\n",
    "X_test5 = scaler.transform(X_test5)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X5 = X_test5[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X5)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "470bc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10000000000000023\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train6 = scaler.fit_transform(X_train6)\n",
    "X_test6 = scaler.transform(X_test6)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X6 = X_test6[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X6)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a02fb4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.10000000000000134\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train7 = scaler.fit_transform(X_train7)\n",
    "X_test7 = scaler.transform(X_test7)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X7 = X_test7[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X7)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "47343059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next bug fixing time is: 0.09999999999999912\n",
      "Fast\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the features again\n",
    "scaler = StandardScaler()\n",
    "X_train8 = scaler.fit_transform(X_train8)\n",
    "X_test8 = scaler.transform(X_test8)\n",
    "\n",
    "# Initializing and training the SVR model\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train1, y_train1)\n",
    "\n",
    "# Making predictions on the first entry of the test set as a demonstration\n",
    "sample_X8 = X_test8[0].reshape(1, -1)\n",
    "predicted_bug_fixing_time = svr_regressor.predict(sample_X8)[0]\n",
    "print(\"Predicted next bug fixing time is:\", predicted_bug_fixing_time)\n",
    "if predicted_bug_fixing_time < 1:\n",
    "    print(\"Fast\")\n",
    "else:\n",
    "        print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bf18c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c390a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3530688158710477\n",
      "Accuracy: 0.7061376317420954\n",
      "Recall: 0.5\n",
      "F-Measure: 0.4138808139534883\n",
      "G-Measure: 0.4138808134683214\n",
      "Matthews correlation coefficient: 0.0\n",
      "Area under curve: 0.5\n",
      "Confusion Matrix:\n",
      " [[1139    0]\n",
      " [ 474    0]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.5\n",
      "Mean Square: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test, y_pred)\n",
    "MCC = matthews_corrcoef(y_test, y_pred)\n",
    "AUC = roc_auc_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8bd11a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train1, y_train1)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0eb9ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4117647058823529\n",
      "Accuracy: 0.8235294117647058\n",
      "Recall: 0.5\n",
      "F-Measure: 0.45161290322580644\n",
      "G-Measure: 0.4516129027304891\n",
      "Matthews correlation coefficient: 0.0\n",
      "Area under curve: 0.5\n",
      "Confusion Matrix:\n",
      " [[42  0]\n",
      " [ 9  0]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.5\n",
      "Mean Square: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test1, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "recall = recall_score(y_test1, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test1, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test1, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test1, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test1, y_pred)\n",
    "MCC = matthews_corrcoef(y_test1, y_pred)\n",
    "AUC = roc_auc_score(y_test1, y_pred)\n",
    "cm = confusion_matrix(y_test1, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d3d64f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train2, y_train2)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6a441b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3630952380952381\n",
      "Accuracy: 0.7261904761904762\n",
      "Recall: 0.5\n",
      "F-Measure: 0.42068965517241375\n",
      "G-Measure: 0.42068965468499403\n",
      "Matthews correlation coefficient: 0.0\n",
      "Area under curve: 0.5\n",
      "Confusion Matrix:\n",
      " [[61  0]\n",
      " [23  0]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.5\n",
      "Mean Square: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test2, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test2, y_pred)\n",
    "recall = recall_score(y_test2, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test2, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test2, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test2, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test2, y_pred)\n",
    "MCC = matthews_corrcoef(y_test2, y_pred)\n",
    "AUC = roc_auc_score(y_test2, y_pred)\n",
    "cm = confusion_matrix(y_test2, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a3db6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train3, y_train3)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "94ccaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5921516754850088\n",
      "Accuracy: 0.5609756097560976\n",
      "Recall: 0.584501347708895\n",
      "F-Measure: 0.5574360341151385\n",
      "G-Measure: 0.5883016407596994\n",
      "Matthews correlation coefficient: 0.1764872887447249\n",
      "Area under curve: 0.584501347708895\n",
      "Confusion Matrix:\n",
      " [[40 13]\n",
      " [41 29]]\n",
      "Normalized Confusion Matrix:\n",
      " [[0.75471698 0.24528302]\n",
      " [0.58571429 0.41428571]]\n",
      "Min: 0.24528301886792453\n",
      "Max: 0.7547169811320755\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.19003641657922873\n",
      "Mean Square: 0.2861138396262742\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test3, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test3, y_pred)\n",
    "recall = recall_score(y_test3, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test3, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test3, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test3, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test3, y_pred)\n",
    "MCC = matthews_corrcoef(y_test3, y_pred)\n",
    "AUC = roc_auc_score(y_test3, y_pred)\n",
    "cm = confusion_matrix(y_test3, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "20d693c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train4, y_train4)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "80e48094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9101796407185629\n",
      "Accuracy: 0.8224852071005917\n",
      "Recall: 0.53125\n",
      "F-Measure: 0.5094814241486068\n",
      "G-Measure: 0.6709074381948165\n",
      "Matthews correlation coefficient: 0.2264342180188771\n",
      "Area under curve: 0.53125\n",
      "Confusion Matrix:\n",
      " [[137   0]\n",
      " [ 30   2]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1.     0.    ]\n",
      " [0.9375 0.0625]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.4697905118241534\n",
      "Mean Square: 0.470703125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test4, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test4, y_pred)\n",
    "recall = recall_score(y_test4, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test4, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test4, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test4, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test4, y_pred)\n",
    "MCC = matthews_corrcoef(y_test4, y_pred)\n",
    "AUC = roc_auc_score(y_test4, y_pred)\n",
    "cm = confusion_matrix(y_test4, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3232a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train5, y_train5)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "32a46413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8920454545454546\n",
      "Accuracy: 0.7888888888888889\n",
      "Recall: 0.5476190476190477\n",
      "F-Measure: 0.526446967599003\n",
      "G-Measure: 0.6786318356496479\n",
      "Matthews correlation coefficient: 0.27326786250001056\n",
      "Area under curve: 0.5476190476190477\n",
      "Confusion Matrix:\n",
      " [[138   0]\n",
      " [ 38   4]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1.        0.       ]\n",
      " [0.9047619 0.0952381]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.45488031367959053\n",
      "Mean Square: 0.4569160997732426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test5, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test5, y_pred)\n",
    "recall = recall_score(y_test5, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test5, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test5, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test5, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test5, y_pred)\n",
    "MCC = matthews_corrcoef(y_test5, y_pred)\n",
    "AUC = roc_auc_score(y_test5, y_pred)\n",
    "cm = confusion_matrix(y_test5, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fbae5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train6, y_train6)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bab69baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3314121037463977\n",
      "Accuracy: 0.6628242074927954\n",
      "Recall: 0.5\n",
      "F-Measure: 0.3986135181975736\n",
      "G-Measure: 0.3986135177181321\n",
      "Matthews correlation coefficient: 0.0\n",
      "Area under curve: 0.5\n",
      "Confusion Matrix:\n",
      " [[230   0]\n",
      " [117   0]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.5\n",
      "Mean Square: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test6, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test6, y_pred)\n",
    "recall = recall_score(y_test6, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test6, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test6, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test6, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test6, y_pred)\n",
    "MCC = matthews_corrcoef(y_test6, y_pred)\n",
    "AUC = roc_auc_score(y_test6, y_pred)\n",
    "cm = confusion_matrix(y_test6, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1414dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train7, y_train7)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "79e404c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.39748953974895396\n",
      "Accuracy: 0.7949790794979079\n",
      "Recall: 0.5\n",
      "F-Measure: 0.4428904428904429\n",
      "G-Measure: 0.4428904423969659\n",
      "Matthews correlation coefficient: 0.0\n",
      "Area under curve: 0.5\n",
      "Confusion Matrix:\n",
      " [[190   0]\n",
      " [ 49   0]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.5\n",
      "Mean Square: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test7, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test7, y_pred)\n",
    "recall = recall_score(y_test7, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test7, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test7, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test7, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test7, y_pred)\n",
    "MCC = matthews_corrcoef(y_test7, y_pred)\n",
    "AUC = roc_auc_score(y_test7, y_pred)\n",
    "cm = confusion_matrix(y_test7, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a748e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train8, y_train8)\n",
    "\n",
    "# Predict outcomes for the test data\n",
    "y_pred = clf_svm.predict(X_test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b1c04730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.33649289099526064\n",
      "Accuracy: 0.6729857819905213\n",
      "Recall: 0.5\n",
      "F-Measure: 0.4022662889518413\n",
      "G-Measure: 0.4022662884709451\n",
      "Matthews correlation coefficient: 0.0\n",
      "Area under curve: 0.5\n",
      "Confusion Matrix:\n",
      " [[284   0]\n",
      " [138   0]]\n",
      "Normalized Confusion Matrix:\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n",
      "Min: 0.0\n",
      "Max: 1.0\n",
      "Average: 0.5\n",
      "Mean: 0.5\n",
      "Standard Deviation: 0.5\n",
      "Mean Square: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test8, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test8, y_pred)\n",
    "recall = recall_score(y_test8, y_pred, average='macro')\n",
    "f_measure_svm = f1_score(y_test8, y_pred, average='macro')\n",
    "\n",
    "def g_measure(y_test8, y_pred):\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test8, y_pred, average='macro')\n",
    "    # Calculate the G-Measure\n",
    "    g_measure = (2 * precision * recall) / (precision + recall + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "    return g_measure\n",
    "\n",
    "G_Measure = g_measure(y_test8, y_pred)\n",
    "MCC = matthews_corrcoef(y_test8, y_pred)\n",
    "AUC = roc_auc_score(y_test8, y_pred)\n",
    "cm = confusion_matrix(y_test8, y_pred)\n",
    "\n",
    "# Normalize confusion matrix to range from 0 to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Statistical measures\n",
    "min_val = np.min(cm_normalized)\n",
    "max_val = np.max(cm_normalized)\n",
    "avg_val = np.mean(cm_normalized)\n",
    "mean_val = np.mean(cm_normalized)\n",
    "std_dev = np.std(cm_normalized)\n",
    "mean_square = np.mean(cm_normalized ** 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Measure:\", f_measure_svm)\n",
    "print(\"G-Measure:\", G_Measure)\n",
    "print(\"Matthews correlation coefficient:\", MCC)\n",
    "print(\"Area under curve:\", AUC)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Normalized Confusion Matrix:\\n\", cm_normalized)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Average:\", avg_val)\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Mean Square:\", mean_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7d3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
